{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22cb3842",
   "metadata": {},
   "source": [
    "# IE105 – Full preprocessing pipeline (author-aligned, train/val/test)\n",
    "Pipeline tổng hợp 4 nguồn dữ liệu (CSV XSS + PortSwigger JSON cheat-sheet + JS library source code từ Materialize & Bootstrap), tiền xử lý, khử trùng lặp, cân bằng theo paper (tuỳ chọn), và chia train/val/test.\n",
    "\n",
    "**Ghi chú**: set `ROOT` trỏ đến thư mục `Dataset` của bạn trên máy local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c5298f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT: D:\\UIT Document\\UIT subjects\\IE105 - Nhập môn đảm bảo và an toàn thông tin\\LLM-for-XSS\\Dataset\n",
      "CSV: D:\\UIT Document\\UIT subjects\\IE105 - Nhập môn đảm bảo và an toàn thông tin\\LLM-for-XSS\\Dataset\\XSS_dataset.csv True\n",
      "JSON_DIR: D:\\UIT Document\\UIT subjects\\IE105 - Nhập môn đảm bảo và an toàn thông tin\\LLM-for-XSS\\Dataset\\xss-cheatsheet-data\\json True\n",
      "MATERIALIZE_DIR: D:\\UIT Document\\UIT subjects\\IE105 - Nhập môn đảm bảo và an toàn thông tin\\LLM-for-XSS\\Dataset\\materialize\n",
      "BOOTSTRAP_DIR: D:\\UIT Document\\UIT subjects\\IE105 - Nhập môn đảm bảo và an toàn thông tin\\LLM-for-XSS\\Dataset\\bootstrap\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import re, json, hashlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# =========================\n",
    "# CONFIG (LOCAL)\n",
    "# =========================\n",
    "# TODO: chỉnh ROOT cho đúng máy bạn\n",
    "ROOT = Path(r\"./Dataset\").resolve()\n",
    "\n",
    "OUT = ROOT / \"_outputs_train_val_test\"\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# ---- Source paths (auto-detect common folder names) ----\n",
    "CSV_PATH = ROOT / \"XSS_dataset.csv\"\n",
    "\n",
    "MATERIALIZE_DIR = None\n",
    "for cand in [\"materialize_js\", \"materialize\"]:\n",
    "    p = ROOT / cand\n",
    "    if p.exists():\n",
    "        MATERIALIZE_DIR = p\n",
    "        break\n",
    "\n",
    "BOOTSTRAP_DIR = None\n",
    "for cand in [\"bootstrap_js\", \"boostrap_js\", \"bootstrap\"]:\n",
    "    p = ROOT / cand\n",
    "    if p.exists():\n",
    "        BOOTSTRAP_DIR = p\n",
    "        break\n",
    "\n",
    "JSON_DIR = None\n",
    "for cand in [\"json\", \"xss-cheatsheet-data/json\", \"xss-cheatsheet-data\\\\json\"]:\n",
    "    p = ROOT / cand\n",
    "    if p.exists():\n",
    "        JSON_DIR = p\n",
    "        break\n",
    "\n",
    "print(\"ROOT:\", ROOT)\n",
    "print(\"CSV:\", CSV_PATH, CSV_PATH.exists())\n",
    "print(\"JSON_DIR:\", JSON_DIR, (JSON_DIR.exists() if JSON_DIR else None))\n",
    "print(\"MATERIALIZE_DIR:\", MATERIALIZE_DIR)\n",
    "print(\"BOOTSTRAP_DIR:\", BOOTSTRAP_DIR)\n",
    "\n",
    "# ---- Normalization ----\n",
    "LOWERCASE = True\n",
    "\n",
    "# ---- Filtering thresholds ----\n",
    "MIN_XSS_CHARS = 6\n",
    "MAX_XSS_CHARS = 2000\n",
    "\n",
    "# JS snippet thresholds (để tạo được nhiều benign samples như paper)\n",
    "MIN_BENIGN_CHARS = 20\n",
    "MAX_BENIGN_CHARS = 350  # snippet-level, không phải file-level\n",
    "\n",
    "# File-level filters\n",
    "DROP_MINIFIED = True\n",
    "EXCLUDE_BOOTSTRAP_TESTS = True\n",
    "\n",
    "# ---- Split ----\n",
    "# Paper: 80/20 train/test\n",
    "TEST_SIZE = 0.20\n",
    "# Nếu cần val: tách từ train (tỉ lệ theo train)\n",
    "MAKE_VAL = True\n",
    "VAL_FRAC_OF_TRAIN = 0.10  # 10% của tập train\n",
    "\n",
    "# ---- Optional: match paper final size exactly (sampling) ----\n",
    "MATCH_PAPER_COUNTS = True\n",
    "TARGET_BENIGN = 12038 if MATCH_PAPER_COUNTS else None\n",
    "TARGET_MAL    = 7321  if MATCH_PAPER_COUNTS else None\n",
    "\n",
    "# ---- Diagnostics ----\n",
    "SHOW_EXAMPLES_PER_LABEL = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2c660aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================\n",
    "# HELPERS\n",
    "# =========================\n",
    "def preprocess_payload(x: str) -> str:\n",
    "    x = \"\" if pd.isna(x) else str(x)\n",
    "    if LOWERCASE:\n",
    "        x = x.lower()\n",
    "    x = x.replace(\"\\r\",\" \").replace(\"\\n\",\" \").replace(\"\\t\",\" \")\n",
    "    x = \" \".join(x.split())\n",
    "    return x\n",
    "\n",
    "def sha1_text(s: str) -> str:\n",
    "    return hashlib.sha1(s.encode(\"utf-8\", errors=\"ignore\")).hexdigest()\n",
    "\n",
    "def read_text_file(path: Path) -> str:\n",
    "    return path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "def is_minified_file(path: Path) -> bool:\n",
    "    # Rule 1: filename contains .min.js\n",
    "    if path.name.endswith(\".min.js\") or \".min.\" in path.name:\n",
    "        return True\n",
    "    # Rule 2: extremely long lines (rough heuristic)\n",
    "    try:\n",
    "        txt = read_text_file(path)\n",
    "    except Exception:\n",
    "        return False\n",
    "    lines = txt.splitlines()\n",
    "    if not lines:\n",
    "        return False\n",
    "    max_len = max(len(l) for l in lines)\n",
    "    avg_len = sum(len(l) for l in lines)/len(lines)\n",
    "    return max_len > 2000 and avg_len > 200\n",
    "\n",
    "def strip_js_comments(text: str) -> str:\n",
    "    # remove /* ... */ and // ... (simple heuristic)\n",
    "    text = re.sub(r\"/\\*.*?\\*/\", \" \", text, flags=re.S)\n",
    "    text = re.sub(r\"(?m)^\\s*//.*?$\", \" \", text)\n",
    "    return text\n",
    "\n",
    "def js_file_filter(path: Path, text: str) -> bool:\n",
    "    if DROP_MINIFIED and is_minified_file(path):\n",
    "        return False\n",
    "    if EXCLUDE_BOOTSTRAP_TESTS and (\"tests\" in path.parts):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# XSS “signal” regex (dùng cho malicious, và dùng để *loại* khỏi benign)\n",
    "XSS_SIGNAL_RE = re.compile(\n",
    "    r\"(<\\s*script\\b|javascript\\s*:|on\\w+\\s*=|document\\s*\\.\\s*cookie|\"\n",
    "    r\"window\\s*\\.\\s*location|alert\\s*\\(|prompt\\s*\\(|confirm\\s*\\(|\"\n",
    "    r\"eval\\s*\\(|settimeout\\s*\\(|setinterval\\s*\\(|<\\s*img\\b|<\\s*svg\\b|\"\n",
    "    r\"src\\s*=|data\\s*:)\",\n",
    "    re.IGNORECASE,\n",
    ")\n",
    "\n",
    "def extract_strings(obj):\n",
    "    out = []\n",
    "    if isinstance(obj, dict):\n",
    "        for v in obj.values():\n",
    "            out.extend(extract_strings(v))\n",
    "    elif isinstance(obj, list):\n",
    "        for v in obj:\n",
    "            out.extend(extract_strings(v))\n",
    "    elif isinstance(obj, str):\n",
    "        out.append(obj)\n",
    "    return out\n",
    "\n",
    "def dedup_and_resolve(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Dedup by hash. If same hash appears with both labels, keep malicious (label=1).\n",
    "    if df.empty:\n",
    "        return df\n",
    "    df = df.copy()\n",
    "    df[\"hash\"] = df[\"payload\"].apply(sha1_text)\n",
    "\n",
    "    conflicts = df.groupby(\"hash\")[\"label\"].nunique()\n",
    "    conflict_hashes = set(conflicts[conflicts > 1].index.tolist())\n",
    "    if conflict_hashes:\n",
    "        df = df[~((df[\"hash\"].isin(conflict_hashes)) & (df[\"label\"] == 0))].copy()\n",
    "\n",
    "    df = df.drop_duplicates(subset=[\"hash\",\"label\"]).reset_index(drop=True)\n",
    "    df = df.drop_duplicates(subset=[\"hash\"]).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def sample_df(df, n, seed=RANDOM_SEED):\n",
    "    if n is None or n >= len(df):\n",
    "        return df.copy()\n",
    "    rng = np.random.default_rng(seed)\n",
    "    idx = rng.choice(len(df), size=n, replace=False)\n",
    "    return df.iloc[idx].copy()\n",
    "\n",
    "def length_stats(series: pd.Series) -> dict:\n",
    "    lens = series.str.len()\n",
    "    return {\n",
    "        \"n\": int(len(series)),\n",
    "        \"mean\": float(lens.mean()),\n",
    "        \"median\": float(lens.median()),\n",
    "        \"p95\": float(lens.quantile(0.95)),\n",
    "        \"max\": float(lens.max())\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af202db",
   "metadata": {},
   "source": [
    "## 1) Load & clean `XSS_dataset.csv` (malicious + benign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e7d1eeb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV clean: (10849, 5)\n",
      "CSV label counts: {1: 7317, 0: 3532}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "assert CSV_PATH.exists(), f\"Không tìm thấy {CSV_PATH}\"\n",
    "\n",
    "df_csv = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Paper dùng cột Sentence/Label; nếu dataset bạn khác, sửa lại ở đây\n",
    "df_csv = df_csv.rename(columns={\"Sentence\":\"payload\", \"Label\":\"label\"})\n",
    "if \"payload\" not in df_csv.columns or \"label\" not in df_csv.columns:\n",
    "    raise ValueError(f\"CSV columns hiện có: {df_csv.columns.tolist()}\")\n",
    "\n",
    "if \"Unnamed: 0\" in df_csv.columns:\n",
    "    df_csv = df_csv.drop(columns=[\"Unnamed: 0\"])\n",
    "\n",
    "df_csv[\"payload\"] = df_csv[\"payload\"].apply(preprocess_payload)\n",
    "df_csv[\"label\"]   = df_csv[\"label\"].astype(int)\n",
    "\n",
    "df_csv = dedup_and_resolve(df_csv)\n",
    "df_csv[\"source\"] = \"kaggle_csv\"\n",
    "df_csv[\"path\"] = str(CSV_PATH)\n",
    "\n",
    "print(\"CSV clean:\", df_csv.shape)\n",
    "print(\"CSV label counts:\", df_csv[\"label\"].value_counts().to_dict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40711c0d",
   "metadata": {},
   "source": [
    "## 2) Load PortSwigger JSON cheat-sheet -> malicious payloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9b90676c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON files found: 21\n",
      "JSON XSS kept: (1276, 5)\n"
     ]
    }
   ],
   "source": [
    "def extract_strings(obj):\n",
    "    out = []\n",
    "    if isinstance(obj, dict):\n",
    "        for v in obj.values():\n",
    "            out.extend(extract_strings(v))\n",
    "    elif isinstance(obj, list):\n",
    "        for v in obj:\n",
    "            out.extend(extract_strings(v))\n",
    "    elif isinstance(obj, str):\n",
    "        out.append(obj)\n",
    "    return out\n",
    "\n",
    "mal_rows = []\n",
    "\n",
    "if JSON_DIR is None:\n",
    "    print(\"JSON_DIR not found; skipping JSON source.\")\n",
    "else:\n",
    "    json_files = sorted(JSON_DIR.rglob(\"*.json\"))\n",
    "    print(\"JSON files found:\", len(json_files))\n",
    "    for jf in json_files:\n",
    "        data = json.loads(jf.read_text(encoding=\"utf-8\", errors=\"ignore\"))\n",
    "        for s in extract_strings(data):\n",
    "            s2 = preprocess_payload(s)\n",
    "            if not s2:\n",
    "                continue\n",
    "            if len(s2) < MIN_XSS_CHARS or len(s2) > MAX_XSS_CHARS:\n",
    "                continue\n",
    "            if not XSS_SIGNAL_RE.search(s2):\n",
    "                continue\n",
    "            mal_rows.append({\"payload\": s2, \"label\": 1, \"source\": f\"portswigger_json:{jf.name}\", \"path\": str(jf)})\n",
    "\n",
    "df_json_xss = pd.DataFrame(mal_rows)\n",
    "if len(df_json_xss):\n",
    "    df_json_xss[\"hash\"] = df_json_xss[\"payload\"].apply(sha1_text)\n",
    "    df_json_xss = df_json_xss.drop_duplicates(subset=[\"hash\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"JSON XSS kept:\", df_json_xss.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae63f1ce",
   "metadata": {},
   "source": [
    "## 3) Load JS library source code -> benign snippets\n",
    "**Cải tiến**: chuyển từ *file-level* sang *snippet-level* để tạo đủ benign samples (paper ~12k)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "937313a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Materialize .js files found: 59\n",
      "Bootstrap .js files found: 128\n",
      "JS benign kept: (27802, 5)\n",
      "JS benign breakdown: {'materialize_js': 17147, 'bootstrap_js': 10655}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def js_to_snippets(text: str):\n",
    "    # Convert a JS file into many short benign snippets.\n",
    "    text = strip_js_comments(text)\n",
    "    raw_lines = [ln.strip() for ln in text.splitlines()]\n",
    "    raw_lines = [ln for ln in raw_lines if ln]\n",
    "\n",
    "    # statement-level candidates\n",
    "    stmts = []\n",
    "    for ln in raw_lines:\n",
    "        parts = [p.strip() for p in ln.split(\";\")]\n",
    "        for p in parts:\n",
    "            if p:\n",
    "                stmts.append(p)\n",
    "\n",
    "    # line-window candidates (context)\n",
    "    K = 3\n",
    "    windows = []\n",
    "    for i in range(0, len(raw_lines) - K + 1):\n",
    "        windows.append(\" \".join(raw_lines[i:i+K]))\n",
    "\n",
    "    candidates = stmts + windows\n",
    "\n",
    "    snippets = []\n",
    "    for c in candidates:\n",
    "        c2 = preprocess_payload(c)\n",
    "        if len(c2) < MIN_BENIGN_CHARS or len(c2) > MAX_BENIGN_CHARS:\n",
    "            continue\n",
    "        if XSS_SIGNAL_RE.search(c2):\n",
    "            continue\n",
    "        if not re.search(r\"\\b(function|const|let|var|return|class|=>|new)\\b\", c2):\n",
    "            if not re.search(r\"[a-zA-Z_]\\w*\\s*\\(|\\w+\\s*\\.\\s*\\w+\", c2):\n",
    "                continue\n",
    "        snippets.append(c2)\n",
    "\n",
    "    return snippets\n",
    "\n",
    "benign_rows = []\n",
    "\n",
    "if MATERIALIZE_DIR is not None:\n",
    "    mat_files = sorted(MATERIALIZE_DIR.rglob(\"*.js\"))\n",
    "    print(\"Materialize .js files found:\", len(mat_files))\n",
    "    for p in mat_files:\n",
    "        txt = read_text_file(p)\n",
    "        if not js_file_filter(p, txt):\n",
    "            continue\n",
    "        for snip in js_to_snippets(txt):\n",
    "            benign_rows.append({\"payload\": snip, \"label\": 0, \"source\": \"materialize_js\", \"path\": str(p)})\n",
    "else:\n",
    "    print(\"Materialize dir not found; skipping.\")\n",
    "\n",
    "if BOOTSTRAP_DIR is not None:\n",
    "    bs_files = sorted(BOOTSTRAP_DIR.rglob(\"*.js\"))\n",
    "    print(\"Bootstrap .js files found:\", len(bs_files))\n",
    "    for p in bs_files:\n",
    "        txt = read_text_file(p)\n",
    "        if not js_file_filter(p, txt):\n",
    "            continue\n",
    "        for snip in js_to_snippets(txt):\n",
    "            benign_rows.append({\"payload\": snip, \"label\": 0, \"source\": \"bootstrap_js\", \"path\": str(p)})\n",
    "else:\n",
    "    print(\"Bootstrap dir not found; skipping.\")\n",
    "\n",
    "df_js_benign = pd.DataFrame(benign_rows)\n",
    "df_js_benign = dedup_and_resolve(df_js_benign) if len(df_js_benign) else df_js_benign\n",
    "\n",
    "print(\"JS benign kept:\", df_js_benign.shape)\n",
    "print(\"JS benign breakdown:\", df_js_benign[\"source\"].value_counts().to_dict() if len(df_js_benign) else {})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e378a4",
   "metadata": {},
   "source": [
    "## 4) Aggregate -> final dataset, optional match paper counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "02a0ce0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined (dedup/resolved): (39155, 5)\n",
      "Label counts: {0: 31334, 1: 7821}\n",
      "Source counts (top 10): {'materialize_js': 17147, 'kaggle_csv': 10849, 'bootstrap_js': 10655, 'portswigger_json:events.json': 147, 'portswigger_json:restricted_characters.json': 78, 'portswigger_json:classic.json': 49, 'portswigger_json:vuejs.json': 43, 'portswigger_json:prototype-pollution.json': 43, 'portswigger_json:angularjs.json': 21, 'portswigger_json:protocols.json': 18}\n",
      "\n",
      "Benign pool: (31334, 5) Malicious pool: (7821, 5)\n",
      "\n",
      "Final dataset: (19359, 5)\n",
      "Final label counts: {0: 12038, 1: 7321}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "parts = [df_csv]\n",
    "if len(df_json_xss):\n",
    "    parts.append(df_json_xss)\n",
    "if len(df_js_benign):\n",
    "    parts.append(df_js_benign)\n",
    "\n",
    "df_all = pd.concat(parts, ignore_index=True)\n",
    "df_all = dedup_and_resolve(df_all)\n",
    "\n",
    "print(\"Combined (dedup/resolved):\", df_all.shape)\n",
    "print(\"Label counts:\", df_all[\"label\"].value_counts().to_dict())\n",
    "print(\"Source counts (top 10):\", df_all[\"source\"].value_counts().head(10).to_dict())\n",
    "\n",
    "benign_pool = df_all[df_all[\"label\"]==0].copy()\n",
    "mal_pool    = df_all[df_all[\"label\"]==1].copy()\n",
    "\n",
    "print(\"\\nBenign pool:\", benign_pool.shape, \"Malicious pool:\", mal_pool.shape)\n",
    "\n",
    "if MATCH_PAPER_COUNTS:\n",
    "    if len(benign_pool) < TARGET_BENIGN:\n",
    "        print(f\"[WARN] Benign pool chưa đủ để match paper: {len(benign_pool)} < {TARGET_BENIGN}.\")\n",
    "    if len(mal_pool) < TARGET_MAL:\n",
    "        print(f\"[WARN] Malicious pool chưa đủ để match paper: {len(mal_pool)} < {TARGET_MAL}.\")\n",
    "    benign_pool = sample_df(benign_pool, min(TARGET_BENIGN, len(benign_pool)))\n",
    "    mal_pool    = sample_df(mal_pool, min(TARGET_MAL, len(mal_pool)))\n",
    "\n",
    "df_final = pd.concat([benign_pool, mal_pool], ignore_index=True)\n",
    "df_final = df_final.sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "\n",
    "print(\"\\nFinal dataset:\", df_final.shape)\n",
    "print(\"Final label counts:\", df_final[\"label\"].value_counts().to_dict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f63725",
   "metadata": {},
   "source": [
    "## 5) Split train/val/test (stratified)\n",
    "Paper chỉ nói train/test 80/20. Nếu bật `MAKE_VAL`, ta tách `VAL_FRAC_OF_TRAIN` từ train để tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "75eb251d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (13938, 2) {0: 8667, 1: 5271}\n",
      "Val: (1549, 2) {0: 963, 1: 586}\n",
      "Test: (3872, 2) {0: 2408, 1: 1464}\n",
      "\n",
      "Saved to: D:\\UIT Document\\UIT subjects\\IE105 - Nhập môn đảm bảo và an toàn thông tin\\LLM-for-XSS\\Dataset\\_outputs_train_val_test\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_final[\"payload\"].astype(str)\n",
    "y = df_final[\"label\"].astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, random_state=RANDOM_SEED, stratify=y\n",
    ")\n",
    "\n",
    "train_df = pd.DataFrame({\"payload\": X_train.values, \"label\": y_train.values})\n",
    "test_df  = pd.DataFrame({\"payload\": X_test.values,  \"label\": y_test.values})\n",
    "\n",
    "if MAKE_VAL:\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "        train_df[\"payload\"], train_df[\"label\"],\n",
    "        test_size=VAL_FRAC_OF_TRAIN, random_state=RANDOM_SEED, stratify=train_df[\"label\"]\n",
    "    )\n",
    "    train_df = pd.DataFrame({\"payload\": X_tr.values, \"label\": y_tr.values})\n",
    "    val_df   = pd.DataFrame({\"payload\": X_val.values, \"label\": y_val.values})\n",
    "\n",
    "print(\"Train:\", train_df.shape, train_df[\"label\"].value_counts().to_dict())\n",
    "if MAKE_VAL:\n",
    "    print(\"Val:\", val_df.shape, val_df[\"label\"].value_counts().to_dict())\n",
    "print(\"Test:\", test_df.shape, test_df[\"label\"].value_counts().to_dict())\n",
    "\n",
    "train_df.to_csv(OUT / \"train.csv\", index=False)\n",
    "if MAKE_VAL:\n",
    "    val_df.to_csv(OUT / \"val.csv\", index=False)\n",
    "test_df.to_csv(OUT / \"test.csv\", index=False)\n",
    "\n",
    "benign_pool.to_csv(OUT / \"benign_pool.csv\", index=False)\n",
    "mal_pool.to_csv(OUT / \"malicious_pool.csv\", index=False)\n",
    "df_final.to_csv(OUT / \"final_dataset.csv\", index=False)\n",
    "\n",
    "print(\"\\nSaved to:\", OUT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e511fd4b",
   "metadata": {},
   "source": [
    "## 6) BoW (CountVectorizer) như paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8bdc9d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW shapes:\n",
      "  train: (13938, 8508)\n",
      "  val  : (1549, 8508)\n",
      "  test : (3872, 8508)\n",
      "Saved vocab: D:\\UIT Document\\UIT subjects\\IE105 - Nhập môn đảm bảo và an toàn thông tin\\LLM-for-XSS\\Dataset\\_outputs_train_val_test\\bow_vocabulary.txt\n",
      "Length stats train: {'n': 13938, 'mean': 85.51499497775865, 'median': 61.0, 'p95': 182.0, 'max': 3478.0}\n",
      "Length stats val: {'n': 1549, 'mean': 83.48482892188508, 'median': 59.0, 'p95': 178.5999999999999, 'max': 3070.0}\n",
      "Length stats test: {'n': 3872, 'mean': 84.79648760330579, 'median': 60.0, 'p95': 180.0, 'max': 3681.0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy import sparse\n",
    "\n",
    "vectorizer = CountVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "\n",
    "Xtr = vectorizer.fit_transform(train_df[\"payload\"])\n",
    "if MAKE_VAL:\n",
    "    Xva = vectorizer.transform(val_df[\"payload\"])\n",
    "Xte = vectorizer.transform(test_df[\"payload\"])\n",
    "\n",
    "print(\"BoW shapes:\")\n",
    "print(\"  train:\", Xtr.shape)\n",
    "if MAKE_VAL:\n",
    "    print(\"  val  :\", Xva.shape)\n",
    "print(\"  test :\", Xte.shape)\n",
    "\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "(vocab_path := OUT / \"bow_vocabulary.txt\").write_text(\"\\n\".join(vocab), encoding=\"utf-8\")\n",
    "print(\"Saved vocab:\", vocab_path)\n",
    "\n",
    "sparse.save_npz(OUT / \"X_train_bow.npz\", Xtr)\n",
    "if MAKE_VAL:\n",
    "    sparse.save_npz(OUT / \"X_val_bow.npz\", Xva)\n",
    "sparse.save_npz(OUT / \"X_test_bow.npz\", Xte)\n",
    "\n",
    "print(\"Length stats train:\", length_stats(train_df[\"payload\"]))\n",
    "if MAKE_VAL:\n",
    "    print(\"Length stats val:\", length_stats(val_df[\"payload\"]))\n",
    "print(\"Length stats test:\", length_stats(test_df[\"payload\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ba8e0e",
   "metadata": {},
   "source": [
    "## 7) Quick sanity-check examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "de951c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Examples label=0 (train):\n",
      "1 <p>wendell wallach introduced the concept of <a href=\"/wiki/artificial_moral_agents\" class=\"mw-redirect\" title=\"artificial moral agents\">artificial moral agents </a> (ama) in his book <i>moral machine...\n",
      "2 parents.push(ancestor) ancestor = ancestor.parentnode.closest(selector) }\n",
      "3 } _selectmenuitem({ key,\n",
      "\n",
      "Examples label=1 (train):\n",
      "1 <picture oncut=\"alert(1)\" contenteditable>test</picture>\n",
      "2 <dir ondblclick=\"alert(1)\">test</dir>\n",
      "3 <object id=x onfocusin=alert(1) type=text/html>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for lbl in [0,1]:\n",
    "    ex = train_df[train_df[\"label\"]==lbl].head(SHOW_EXAMPLES_PER_LABEL)[\"payload\"].tolist()\n",
    "    print(f\"\\nExamples label={lbl} (train):\")\n",
    "    for i,e in enumerate(ex,1):\n",
    "        print(i, (e[:200] + (\"...\" if len(e)>200 else \"\")))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
